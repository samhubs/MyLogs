# Challenge
A compilation of my journey working through the problems on TorchLeet covering day count, task, gaps, key learnings and references

# Progress table

A structured, day-by-day track of "what was done". Each day includes:

---

| **Day** | **Task** | **Gaps** | **Key Learnings** | **Resource Suggestions**|
|:------:|:-----:|:--------:|:-------:|:---:|
| **1**  | Implement Linear Regression (Easy)| Custom loss function definition |- Linear regression is linear in parameters and not necessarily in the input features, a perfectly fine linear regression model $\beta _1 X_1 + \beta _2 {X_1}^2 + \beta_3 X_1 X_2$ <br /> - For multiple output regression, multiple heads are used | |
|**2**| Write a custom Dataset and Dataloader to load from a CSV file| Ensure `dtype` of the input data and the model weights are the same, Dataset` and `DataLoader` reside in `torch.utils.data`, achieve train/test split, an element of a `DataLoader` can be extracted with `next(iter(` |- The instantiation of a `CustomDataset` class is a way to store the features and targets separately for the `__len__` and `__getitem__` methods, `__getitems__` can return any number of variables|[Pytorch official documentation](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)|
|**3**| Write a Custom Activation Function| Learnable parameters in pytorch, extracting model parameters through `model.parameters()`| Parameters defined with `nn` module are learnable, a custom function defined as `torch.tanh(x) + x` has no learnable ones||
|**4**| Implement Custom Loss Function (Huber Loss) | Performing element-wise `if` and `else` supported conditional operations on tensors, e.g. if the difference of two `1D` vectors is less than a scalar value, threshold for Huber loss| Use `map` and `torch.where` to perform the element-wise check, the `loss` class whether a custom or a built-in, they return a scalar value, typically mean|[Pytorch.where](https://pytorch.org/docs/stable/generated/torch.where.html)|
|**5**| Implement a Deep Neural Network| The `__getitem__` method return structure, `nn.Sequential` as a network, how to plot a surface plot to show the input data, difference between `unsqueeze` and `view`|For a custom dataset class, the `__getitem__` method return a training example and its corresponding label at a given index, a custom loss is typically defined with a `nn.Module` and has a `forward` method that expects predicitons and true labels and returns a scalar value|[Excellent discussion on view and unsqueeze](https://discuss.pytorch.org/t/what-is-the-difference-between-view-and-unsqueeze/1155/6)|
|**6**|Visualize Training Progress with TensorBoard in PyTorch|`tensorboard` setup and concepts, methods of `SummaryWriter`|`tensorboard` is a great ML observability tool, lets one track ML runs and their vitals.|[Neptune Tensorboard Tutorial](https://neptune.ai/blog/tensorboard-tutorial), [SummaryWriter Class](https://github.com/pytorch/pytorch/blob/v2.6.0/torch/utils/tensorboard/writer.py#L172)|
| **7** | Save and Load Your PyTorch Model | `state_dict()`, parameters and how to access them, saving a full model and just `state_dict()`| `state_dict()` is an ordered dictionary containing all the model parameters and their corresponding values that can be extracted with `for params, val in model.state_dict().items():` |[Saving and loading Pytorch models](http://pytorch.org/tutorials/beginner/saving_loading_models.html)|
| **8** | Implement a LSTM Model | Difference between `time_step, batch_size, sequence_length, test_sequence`, hidden states|`time_step`: single position in time, `sequence_length`: number of timesteps in a training example, `batch_size`: number of training examples, `test_sequence`: test batch for prediction and visualization, LSTMs keep track of various processing states of an input through 3 gates: `input`, `forget` and `output`, hidden state is computed through these gates at each time, and the previous hidden state used as input along with the current sequence input |[LSTM Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) <br> [Tutorial from d2l book](https://d2l.ai/chapter_recurrent-modern/lstm.html)|