{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"Deep%20Learning%20CMU/module1/","title":"Module 1","text":"<p>This document covers my journey with the deep learning course. I intend to use Deep Dive book and Prof. Raj lectures on Youtube. I am using this Mkdoc to help me with my journey</p>"},{"location":"Deep%20Learning%20CMU/module2/","title":"A. Softmax Function and LogSumExp Trick","text":""},{"location":"Deep%20Learning%20CMU/module2/#overview","title":"Overview","text":"<p>The discussion focused on the softmax function, its implementation using the LogSumExp trick, and how this trick enhances numerical stability. The softmax function is essential in machine learning for converting real-valued vectors into probability distributions. However, direct implementation can lead to numerical issues like overflow and underflow, especially when dealing with large input values.</p>"},{"location":"Deep%20Learning%20CMU/module2/#softmax-function","title":"Softmax Function","text":"<ul> <li>Purpose: Used in classification tasks to convert a vector of real numbers into a probability distribution.</li> <li>Formula: Softmax of an element $ x_i $ in a vector $ x $ is defined as \\(\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\\)</li> <li>Batch Processing: In practical applications, especially in neural networks, softmax is often applied to batches of data, requiring the function to handle 2D arrays.</li> </ul>"},{"location":"Deep%20Learning%20CMU/module2/#logsumexp-trick","title":"LogSumExp Trick","text":"<ul> <li>Problem Addressed: Direct computation of softmax can lead to numerical instability due to exponentiation of large numbers.</li> <li>Solution: The LogSumExp trick improves numerical stability by subtracting the maximum element in the input vector from each element before exponentiation.</li> <li>Implementation:</li> <li>Compute the maximum value \\(a\\) in the input array (or along each row for a 2D array).</li> <li>Adjust the softmax calculation to $ \\text{softmax}(x_i) = \\frac{e^{x_i - a}}{\\sum_{j} e^{x_j - a}}$</li> <li>This adjustment ensures no overflow or underflow during computation.</li> </ul>"},{"location":"Deep%20Learning%20CMU/module2/#softmax-with-logsumexp-trick-implementation","title":"Softmax with LogSumExp Trick Implementation","text":"<ul> <li>Input: A 2D array, typically representing a batch of data points.</li> <li>Process:</li> <li>Find the maximum value \\( a \\) in each row.</li> <li>Compute <code>denom</code> as $ a + \\log(\\sum_{j} e^{x_j - a}) $ for each row.</li> <li>Apply softmax by calculating <code>np.exp(x - denom)</code>, ensuring each row sums up to 1.</li> </ul> <p>source: Programming Assignment 2.1: Multilayer Perceptron (Assignment 1A) - https://classroom.emeritus.org/courses/6844 <pre><code>def softmax(x):\n    \"\"\"[Given] Calculates the softmax of the input array using the LogSumExp trick for numerical stability.\n    Args:\n        x (np.array): Input array, shaped (batch_size, d), where d is any integer.\n    Returns:\n        np.array: Same shape as input, but the values of each row are now scaled to add up to 1.\n    \"\"\"\n    # [Given] Use this in CrossEntropyLoss.\n    a = np.max(x, axis=1, keepdims=True)\n    denom = a + np.log(np.sum(np.exp(x - a), axis=1, keepdims=True))\n    return np.exp(x - denom)\n</code></pre> - Benefits: This implementation is numerically stable and suitable for batch processing in machine learning contexts.</p>"},{"location":"Deep%20Learning%20CMU/module2/#example-and-practical-application","title":"Example and Practical Application","text":"<ul> <li>Use Case: Applied in machine learning models, particularly in calculating probabilities for classification tasks and in loss functions like cross-entropy loss.</li> <li>Numerical Stability: The LogSumExp trick ensures the softmax function works accurately even with high-dimensional data and large values, common in deep learning models.</li> </ul>"},{"location":"MLOPs/Ml_Airflow/","title":"ML pipeline with Airflow","text":""},{"location":"MLOPs/RestAPIs/","title":"RestAPI, Flask, Postman","text":"<p>Let us understand the whole ecosystem of how to build, document, and test APIs using a simple example based on Python. Steps to follow:</p> <ol> <li> <p>Install Flask using pip \\ <code>pip install flask</code></p> </li> <li> <p>Create an app.py file using flask that defines two endpoints, we will test <code>GET</code> and <code>POST</code> requests: one for listing tasks and another for adding tasks.</p> </li> </ol> <pre><code>from flask import Flask, request, jsonify\n\napp = Flask(__name__)\n\n# Sample data to store tasks\ntasks = []\n\n@app.route('/tasks', methods=['GET'])\ndef get_tasks():\n    return jsonify(tasks)\n\n@app.route('/tasks', methods=['POST'])\ndef add_task():\n    data = request.get_json()\n    if 'task' in data:\n        new_task = {'task': data['task']}\n        tasks.append(new_task)\n        return jsonify(new_task), 201\n    else:\n        return jsonify({'error': 'Task field is required'}), 400\n\nif __name__ == '__main__':\n    app.run(debug=True)\n</code></pre> <ol> <li> <p>Run <code>app.py</code> \\ This is the output you will see on <code>localhost:5000\\tasks</code> url. </p> </li> <li> <p>Go to <code>web.postman.co</code> and create a new request. For a GET request, type <code>localhost:5000\\tasks</code> in the URL and hit <code>Send</code>. You will see the following output with a 200 success code: \\</p> </li> </ol> <p> </p> <ol> <li>Select <code>POST</code>, go to <code>Headers</code> and add a key-value pair <code>Content-type:application/json</code>. In addition, go to <code>Body</code> and add the following <pre><code>{\n    \"task\":\"Simple Test\"\n}\n</code></pre> Hit <code>Send</code>, you should see the results as follows:</li> </ol> <p></p> <p>This suggests that the task list has been appended with <code>Simple Test</code>. You can verify that by sending a <code>Get</code> request agai</p> <p></p>"},{"location":"MLOPs/guardianwatch_lambda/numpy/random/LICENSE/","title":"LICENSE","text":"<p>This software is dual-licensed under the The University of Illinois/NCSA Open Source License (NCSA) and The 3-Clause BSD License</p>"},{"location":"MLOPs/guardianwatch_lambda/numpy/random/LICENSE/#ncsa-open-source-license","title":"NCSA Open Source License","text":"<p>Copyright (c) 2019 Kevin Sheppard. All rights reserved.</p> <p>Developed by: Kevin Sheppard (kevin.sheppard@economics.ox.ac.uk, kevin.k.sheppard@gmail.com) http://www.kevinsheppard.com</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers.</p> <p>Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution.</p> <p>Neither the names of Kevin Sheppard, nor the names of any contributors may be used to endorse or promote products derived from this Software without specific prior written permission.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.</p>"},{"location":"MLOPs/guardianwatch_lambda/numpy/random/LICENSE/#3-clause-bsd-license","title":"3-Clause BSD License","text":"<p>Copyright (c) 2019 Kevin Sheppard. All rights reserved.</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <ol> <li> <p>Redistributions of source code must retain the above copyright notice,    this list of conditions and the following disclaimer.</p> </li> <li> <p>Redistributions in binary form must reproduce the above copyright notice,    this list of conditions and the following disclaimer in the documentation    and/or other materials provided with the distribution.</p> </li> <li> <p>Neither the name of the copyright holder nor the names of its contributors    may be used to endorse or promote products derived from this software    without specific prior written permission.</p> </li> </ol> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"MLOPs/guardianwatch_lambda/numpy/random/LICENSE/#components","title":"Components","text":"<p>Many parts of this module have been derived from original sources,  often the algorithm's designer. Component licenses are located with  the component code.</p>"},{"location":"Tidbits-knowledge/Binomial%20Distribution/","title":"Binomial Distribution with examples","text":"<p>I was using the <code>numpy.random.binomial</code> function and started to wonder if I understood the intuition behind the Binomial distribution - I didn't. </p> <p>Let's join hands and explore Binomial Distribution with practical examples. Here is our toy example:</p> <p>Suppose we have a bag containing 5 yellow and 4 green balls. For the following questions, let's assume that each time you draw a ball, you replace it, ensuring the probabilities remain constant for each draw (this is an important aspect for the scenario to fit a binomial distribution). Based on the above, I came up with a set of questions covering various scenarios, e.g. exact number of success (yellow ball draw in this case), a range on the number of successes (at least 5, more than 6, etc.), testing symmetry of the binomial coefficients. Let's go and answer them:</p> <ol> <li> <p>Probability of Drawing a Specific Number of Yellow Balls in a Fixed Number of Draws Question: If you draw a ball from the bag 10 times (with replacement), what is the probability of getting exactly 6 yellow balls?  Answer: The probability is calculated using the binomial distribution formula:       \\(P(X = 6) = \\binom{10}{6} \\left(\\frac{5}{9}\\right)^6 \\left(\\frac{4}{9}\\right)^4\\)</p> </li> <li> <p>Comparing Probabilities for Different Numbers of Successes Question: In 8 draws, which is more likely - drawing exactly 3 yellow balls or exactly 5 yellow balls? Answer: </p> <ul> <li>Probability of 3 Yellow Balls:     \\(P(X = 3) = \\binom{8}{3} \\left(\\frac{5}{9}\\right)^3 \\left(\\frac{4}{9}\\right)^5\\)</li> <li>Probability of 5 Yellow Balls:     \\(P(X = 5) = \\binom{8}{5} \\left(\\frac{5}{9}\\right)^5 \\left(\\frac{4}{9}\\right)^3\\)</li> </ul> </li> <li> <p>Probability of Drawing At Least a Certain Number of Yellow Balls Question: What is the probability of drawing at least 4 yellow balls in 7 draws? Answer: The probability is the sum of probabilities of drawing 4, 5, 6, and 7 yellow balls:       \\(P(X \\geq 4) = \\sum_{k=4}^{7} \\binom{7}{k} \\left(\\frac{5}{9}\\right)^k \\left(\\frac{4}{9}\\right)^{7-k}\\)</p> </li> <li> <p>Effect of Changing the Number of Trials on Probability Question: How does the probability of drawing exactly 2 yellow balls change when you increase the number of draws from 5 to 10? Answer: </p> <ul> <li>Probability in 5 Draws:     \\(P(X = 2) = \\binom{5}{2} \\left(\\frac{5}{9}\\right)^2 \\left(\\frac{4}{9}\\right)^3\\)</li> <li>Probability in 10 Draws:     \\(P(X = 2) = \\binom{10}{2} \\left(\\frac{5}{9}\\right)^2 \\left(\\frac{4}{9}\\right)^8\\)</li> </ul> </li> <li> <p>Varying the Probability of Success Question: If one yellow ball is removed from the bag (making it 4 yellow and 4 green), how does this change the probability of drawing exactly 3 yellow balls in 6 draws? Answer: With the new probability of a yellow ball being \\( p' = \\frac{4}{8} = \\frac{1}{2} \\), the probability is:       \\(P(X = 3) = \\binom{6}{3} \\left(\\frac{1}{2}\\right)^3 \\left(\\frac{1}{2}\\right)^3\\)</p> </li> <li> <p>Combining Successes and Failures Question: In 12 draws, what is the probability of drawing exactly 7 yellow balls and 5 green balls? Answer: The probability is calculated as:       \\(P(X = 7) = \\binom{12}{7} \\left(\\frac{5}{9}\\right)^7 \\left(\\frac{4}{9}\\right)^5\\)</p> </li> </ol>"},{"location":"Tidbits-knowledge/Binomial%20Distribution/#additional-discussion","title":"Additional Discussion","text":"<p>Symmetry in Binomial Coefficients   - Observation: The binomial coefficients for certain combinations, like 7 successes out of 10 trials and 3 successes out of 10 trials, are the same.   - Reason: This symmetry arises because choosing \\(k\\) successes out of \\(n\\) trials is equivalent to choosing \\(n-k\\) failures, leading to the equality      $ \\binom{n}{k} = \\binom{n}{n-k} $   - Significance: This symmetry reflects the dual nature of success and failure in binomial situations and simplifies calculations by allowing the focus on the smaller of the two numbers (successes or failures).</p> <p>In summary, we covered a comprehensive overview of the binomial distribution through a series of questions and answers related to drawing balls from a bag containing yellow and green balls. We covered concepts such as calculating the probability of drawing a specific number of yellow balls, comparing probabilities for different outcomes, and the impact of varying trial numbers and success probabilities. The significance and symmetry of binomial coefficients were also explained, highlighting the dual nature of success and failure in binomial situations. </p>"},{"location":"Tidbits-knowledge/Monocular3DMeasurements/","title":"Monocular3DMeasurements","text":"<p>Monocular 3D measurements</p> <p>Monocular depth estimation is an important tool for letting users measure any object in a given photo. This is how it can be achieved:</p> <ol> <li> <p>Place a reference object in the scene and take an image of the object standing heads-on</p> </li> <li> <p>Use a deep learning approach to get the depth map - using a technique such as Marigold (https://arxiv.org/pdf/2312.02145.pdf)</p> </li> <li> <p>Determine the Tilt Angle of the calibration target:</p> </li> </ol> <p>Assuming the plate is vertical and the camera is level, the difference in depth between the top and bottom points corresponds to the physical height difference due to the tilt.</p> <p>The actual height of the plate is known (8 inches in your case).</p> <p>Using trigonometry, the tilt angle \u03b8 can be estimated. If d is the difference in depth between top and bottom \\(\\|Z_{\\text{bottom}} - Z_{\\text{top}}\\|\\) and \u210e is the actual height of the plate, then: </p> \\[ \\theta = \\arctan\\frac{b}{h} \\] <ol> <li>Correcting Perspective Distortion</li> </ol> <p>To correct perspective distortion, we need to transform the coordinates in the image to what they would be if viewed head-on. This involves several steps:</p> <pre><code>a. Homography Matrix\n</code></pre> <p>A homography is a 3x3 transformation matrix that maps points from one plane to another. It's used in perspective correction.</p> <pre><code>b. Points Selection\n</code></pre> <p>Select at least four corresponding points on the object in the image and what their coordinates would be without distortion. These points should not be co-linear and should form a rectangle or square for an object like a flat plate.</p> <pre><code>c. Calculating the Homography Matrix\n</code></pre> <p>The homography matrix H can be calculated using these point correspondences. This involves solving a set of linear equations derived from the point correspondences.</p> <pre><code>d. Applying the Transformation\n</code></pre> <p>Once H is obtained, it's applied to the image or the object in the image to correct the perspective distortion. This transformation maps the coordinates from the distorted image to their undistorted locations.</p> <ol> <li>Calculate the Conversion Factor</li> </ol> <p>With the corrected dimensions of the plate in pixels, compare these to the actual dimensions (8\"x8\") to calculate the conversion factor.</p> <ol> <li>Coordinate Transformation</li> </ol> <p>Suppose we want to measure the width of the table, and we've identified two points on the table's edges in the depth map at pixel coordinates </p> <p>\\((x_1, y_1) \\text{ and } (x_2, y_2)\\)</p> <p>Let's say the depth values at these points are \\(Z_1\\) and \\(Z_2\\) meters. To convert these to 3D coordinates, we use the formula:</p> \\[ X = \\frac{(x - c_x) \\times Z}{f} $$ $$ Y = \\frac{(y - c_y) \\times Z}{f} \\] <p>So, for both points, we calculate \\((X_1, Y_1, Z_1)\\) and \\((X_2, Y_2, Z_2)\\).</p> <p>Note: Make sure Z above is \\(Z_{pixel} \\times k\\) where \\(k\\) is the correction factor.</p> <ol> <li>Measurement</li> </ol> <p>We measure the distance between these two points in 3D space using the Euclidean distance formula:</p> \\[\\text{Distance} = \\sqrt{(X_2 - X_1)^2 + (Y_2 - Y_1)^2 + (Z_2 - Z_1)^2}\\] <p>This way one can potentially measure <code>metric</code> distances between points given a single image. </p>"},{"location":"Tidbits-knowledge/init_discussion/","title":"Are you __init__ or not?","text":""},{"location":"Tidbits-knowledge/init_discussion/#introduction","title":"Introduction","text":""},{"location":"Tidbits-knowledge/init_discussion/#python-packages-__init__py-vs-implicit-namespace-packages","title":"Python Packages: <code>__init__.py</code> vs. Implicit Namespace Packages","text":"<p>In Python, packages are a way to organize and structure code. Two common approaches for organizing code into packages are using <code>__init__.py</code> files and <code>implicit namespace packages</code>. This document explores both approaches using a simple example.</p>"},{"location":"Tidbits-knowledge/init_discussion/#toy-example","title":"Toy Example","text":"<p>Consider a package named <code>my_package</code> that contains two subpackages, <code>subpkg1</code> and <code>subpkg2</code>, each with their own modules. We want to organize this structure and make the modules accessible when we import <code>my_package</code>.</p>"},{"location":"Tidbits-knowledge/init_discussion/#directory-structure","title":"Directory Structure","text":"<p><pre><code>my_package\n    __init__.py\n    subpkg1\n        __init__.py\n        module1.py\n    subpkg2\n        __init__.py\n        module2.py\n</code></pre> Let's import the submodules in <code>my_package</code> using the two methods and compare their functionalities.</p>"},{"location":"Tidbits-knowledge/init_discussion/#a-using-__init__py-files","title":"A) Using <code>__init__.py</code> Files","text":"<ol> <li>Let's see what's inside my_package <code>__init__.py</code>\\ In the my_package <code>__init__.py</code> file, we can use <code>__init__.py</code> files to organize the code and control what is imported when my_package is imported.</li> </ol> <pre><code># my_package/__init__.py\n\n# Import submodules when the package is imported\nfrom .subpkg1 import module1\nfrom .subpkg2 import module2\n\n# You can also define package-level functions or variables here\n</code></pre> <p>With this approach:</p> <ul> <li><code>module1</code> and <code>module2</code> from <code>subpkg1</code> and <code>subpkg2</code> are automatically imported when <code>my_package</code> is imported.</li> <li>The package's namespace is organized and controlled within the init.py file.</li> <li>The init files within the subpackages serves the same purpose as shown below:</li> </ul> <pre><code># my_package/subpkg1/__init__.py\n# This file defines behavior for subpkg1 and imports submodules within subpkg1.\n\n# Import submodules within subpkg1\nfrom . import module1\n\n# You can also define subpackage-level functions or variables here\n\n# my_package/subpkg2/__init__.py\n# This file defines behavior for subpkg2 and imports submodules within subpkg2.\n\n# Import submodules within subpkg2\nfrom . import module2\n</code></pre>"},{"location":"Tidbits-knowledge/init_discussion/#b-using-implicit-namespace-packages","title":"B) Using Implicit Namespace Packages","text":""},{"location":"Tidbits-knowledge/init_discussion/#no-__init__py-files","title":"<code>No __init__.py</code> Files","text":"<p>With implicit namespace packages, there are no <code>__init__.py</code> files. The directory structure remains the same, but you don't create <code>__init__.py</code> files in each subpackage.</p> <pre><code>my_package/\n    subpkg1/\n        module1.py\n    subpkg2/\n        module2.py\n</code></pre> <p>To create an implicit namespace package, you simply have the package directories without init.py files.</p> <p><code>module1.py</code> and <code>module2.py</code> You can place your module files directly within the subpackage directories (<code>subpkg1</code> and <code>subpkg2</code>) without the need for <code>__init__.py</code> files.</p> <p>With this approach:</p> <ul> <li>Modules <code>module1</code> and <code>module2</code> are directly within their respective subpackage directories.</li> <li>There are no <code>__init__.py</code> files.</li> <li>The package structure is created implicitly without the need for <code>__init__.py</code> files.</li> </ul>"},{"location":"Tidbits-knowledge/init_discussion/#choosing-between-__init__py-and-implicit-namespace-packages","title":"Choosing Between <code>__init__.py</code> and <code>Implicit Namespace Packages</code>","text":"<p>The choice between using the two depends on your project's requirements and your preferred organization style. Here are some considerations:</p> <ul> <li> <p><code>__init__.py</code> Files: Useful for explicit control over imports, organizing code, and defining package-level variables/functions. Provides a clear namespace.</p> </li> <li> <p><code>Implicit Namespace Packages</code>: Simplifies package creation and organization by omitting <code>__init__.py</code> files. Suitable for smaller, loosely organized projects.</p> </li> </ul> <p>Ultimately, the choice is yours, and you can use the approach that best fits your project's needs.</p>"},{"location":"TorchLeet/torchleet_readme/","title":"Challenge","text":"<p>A compilation of my journey working through the problems on TorchLeet covering day count, task, gaps, key learnings and references</p>"},{"location":"TorchLeet/torchleet_readme/#progress-table","title":"Progress table","text":"<p>A structured, day-by-day track of \"what was done\". Each day includes:</p> Day Task Gaps Key Learnings Resource Suggestions 1 Implement Linear Regression (Easy) Custom loss function definition - Linear regression is linear in parameters and not necessarily in the input features, a perfectly fine linear regression model \\(\\beta _1 X_1 + \\beta _2 {X_1}^2 + \\beta_3 X_1 X_2\\)  - For multiple output regression, multiple heads are used 2 Write a custom Dataset and Dataloader to load from a CSV file Ensure <code>dtype</code> of the input data and the model weights are the same, Dataset<code>and</code>DataLoader<code>reside in</code>torch.utils.data<code>, achieve train/test split, an element of a</code>DataLoader<code>can be extracted with</code>next(iter(<code>|- The instantiation of a</code>CustomDataset<code>class is a way to store the features and targets separately for the</code>len<code>and</code>getitem<code>methods,</code>getitems` can return any number of variables Pytorch official documentation 3 Write a Custom Activation Function Learnable parameters in pytorch, extracting model parameters through <code>model.parameters()</code> Parameters defined with <code>nn</code> module are learnable, a custom function defined as <code>torch.tanh(x) + x</code> has no learnable ones 4 Implement Custom Loss Function (Huber Loss) Performing element-wise <code>if</code> and <code>else</code> supported conditional operations on tensors, e.g. if the difference of two <code>1D</code> vectors is less than a scalar value, threshold for Huber loss Use <code>map</code> and <code>torch.where</code> to perform the element-wise check, the <code>loss</code> class whether a custom or a built-in, they return a scalar value, typically mean Pytorch.where 5 Implement a Deep Neural Network The <code>__getitem__</code> method return structure, <code>nn.Sequential</code> as a network, how to plot a surface plot to show the input data, difference between <code>unsqueeze</code> and <code>view</code> For a custom dataset class, the <code>__getitem__</code> method return a training example and its corresponding label at a given index, a custom loss is typically defined with a <code>nn.Module</code> and has a <code>forward</code> method that expects predicitons and true labels and returns a scalar value Excellent discussion on view and unsqueeze 6 Visualize Training Progress with TensorBoard in PyTorch <code>tensorboard</code> setup and concepts, methods of <code>SummaryWriter</code> <code>tensorboard</code> is a great ML observability tool, lets one track ML runs and their vitals. Neptune Tensorboard Tutorial, SummaryWriter Class 7 Save and Load Your PyTorch Model <code>state_dict()</code>, parameters and how to access them, saving a full model and just <code>state_dict()</code> <code>state_dict()</code> is an ordered dictionary containing all the model parameters and their corresponding values that can be extracted with <code>for params, val in model.state_dict().items():</code> Saving and loading Pytorch models 8 Implement a LSTM Model Difference between <code>time_step, batch_size, sequence_length, test_sequence</code>, hidden states <code>time_step</code>: single position in time, <code>sequence_length</code>: number of timesteps in a training example, <code>batch_size</code>: number of training examples, <code>test_sequence</code>: test batch for prediction and visualization, LSTMs keep track of various processing states of an input through 3 gates: <code>input</code>, <code>forget</code> and <code>output</code>, hidden state is computed through these gates at each time, and the previous hidden state used as input along with the current sequence input LSTM Pytorch Tutorial from d2l book"}]}